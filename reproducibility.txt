# LSTM
; No mention of any approach to OOV tokens 
; batch-size(64) and hidden_units(200) taken from https://github.com/jimmyyfeng/TD-LSTM/blob/master/lstm.py
-em=twitter-100[corpus] -ds=dong2 -m=lstm -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=lstm_100
-em=twitter-200[corpus] -ds=dong2 -m=lstm -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=lstm_200

# TDLSTM
; No mention of any approach to OOV tokens 
; batch-size(200) and hidden_units(200) taken from https://github.com/jimmyyfeng/TD-LSTM/blob/master/td_lstm.py
-em=twitter-100[corpus] -ds=dong2 -m=tdlstm -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=tdlstm_100
-em=twitter-200[corpus] -ds=dong2 -m=tdlstm -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=tdlstm_200

# TCLSTM
; No mention of any approach to OOV tokens 
; batch-size(64) and hidden_units(200) taken from https://github.com/jimmyyfeng/TD-LSTM/blob/master/tc_lstm.py
-em=twitter-100[corpus] -ds=dong2 -m=tclstm -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=tclstm_100
-em=twitter-200[corpus] -ds=dong2 -m=tclstm -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=tclstm_200

# MemNet
; No mention of any approach to OOV tokens 
; glove-300d-commoncrawl-42 version is identified by quoting vocab size of 1.9M
; Using location model #2 since this reported best results
; implies that word embeddings are not trained
; batch-size(100) taken from https://github.com/NUSTM/ABSC/blob/master/models/ABSC_Zozoz/model/dmn.py
-em=commoncrawl-42[corpus] -ds=restaurants -m=memnet -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=memnet_restaurants
-em=commoncrawl-42[corpus] -ds=laptops -m=memnet -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=memnet_laptops

# IAN
; OOV words initialized to using U(-0.1,0.1) 
; batch-size(64) taken from default at https://github.com/songyouwei/ABSA-PyTorch/blob/master/train.py
; No mention which version of glove-300d is used, authors cite Wang-ATAE when quoting LSTM hidden units, assuming same GloVe version used. 
-em=commoncrawl-840[corpus] -ds=restaurants -m=ian -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=ian_restaurants
-em=commoncrawl-840[corpus] -ds=laptops -m=ian -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=ian_laptops

# RAM
; OOV words just mentioned in the end re: experimenting with training embeddings or not, implies oov words are initialized with random vectors
; paper simply states "initialized randomly" no specification of random uniform parameters.
; batch-size, lstm_hidden_units and gru_hidden_units taken from https://github.com/lpq29743/RAM/blob/master/main.py
; glove-300d-commoncrawl-42 version is identified by quoting vocab size of 1.9M
-em=commoncrawl-42[corpus] -ds=dong2 -m=ram -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=ram_tweets
-em=commoncrawl-42[corpus] -ds=restaurants -m=ram -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=ram_restaurants
-em=commoncrawl-42[corpus] -ds=laptops -m=ram -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=ram_laptops

# LCR-ROT
; OOV words initialized to using U(-0.1,0.1) 
; batch-size is taken from example posted on https://github.com/NUSTM/ABSC/tree/master/models/ABSC_Zozoz
; Conflicting report says they used same glove-300d as Tang-MemNet(who uses commoncrawl-42) and Wang-ATAE(who uses commoncrawl-840), assuming latter since larger
-em=commoncrawl-840[corpus] -ds=dong2 -m=lcrrot -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=lcrrot_tweets
-em=commoncrawl-840[corpus] -ds=restaurants -m=lcrrot -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=lcrrot_restaurants
-em=commoncrawl-840[corpus] -ds=laptops -m=lcrrot -s=10000 -cmt=T1NrVnn32dXWeOxeQWGArkHwc -mp oov=True -aux logging=true -contd=lcrrot_laptops